{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wrangling Report\n",
    "\n",
    "#### Gather\n",
    "Acquiring the WeRateDogs data was straightforward. The enhanced archive and image prediction data were immediately available. Accessing the Twitter API required jumping through a few hoops, but the online instructions and tweepy module streamlined the process as much as could be hoped. Querying the API took awhile, and some tweet IDs from the enhanced archive didn't have matching records. Because of that (and for fear of having my API privileges revoked), I ran that process only once, and used the locally stored API data thereafter.\n",
    "\n",
    "Getting the API data into a pandas dataframe posed some issues. The API data is nested JSON, representing a few different Twitter objects: Tweets, Entities (images, links, hashtags), Extened Entities (more media), and Users. For the initial examination, I chose to load in only Tweet and User data, leaving Entity data for future analysis.\n",
    "\n",
    "#### Assess\n",
    "Most of the assessment was spent on the enhanced archive. All the tweet IDs are unique, each corresponding to one row in the data, which made the rest of the assessment less painful. There are a number of predictable issues with this data: data with incorrect datatype, data embedded inside other data, null-value handling, etc. Slightly more complicated was a tidyness issue: \"dog stage\" information was split across 4 columns, rather than being shared in 1 column.\n",
    "\n",
    "The image prediction data was mostly clean, except that some image URLs where shared by different tweets. This might not be incorrect, but warrants more investigation. Other issues were somewhere in between quality and tidyness: is a row with only incorrect predictions worth retaining? Is a row with greater than 100% confidence in its predictions valid? Additionally, each row contained multiple predictions. A more pure treatment would would have only 1 prediction per line. However, that might be overkill for analysis.\n",
    "\n",
    "Data from the Twitter API was also clean, ignoring the nesting issues handled in the gathering step. There was a fair amount of missing and repeated data: no geographic information, and the User data retained in gathering was useless. All the relevant tweets have the same user: the WeRateDogs account. This prohibits interesting areas of analysis, but doesn't require much handling. There is another apparent anamoly: some tweets with data for favorite_count and retweet_count didn't have a corresponding boolean value in favorited or retweeted. Again, this is not necessarily incorrect, but should be investigated before using in analysis.\n",
    "\n",
    "#### Cleaning\n",
    "I made the decision to the focus on the enhanced archive. Rather than clean the image prediction and API data, I decided instead to corral the subset of information that looked useful, and leave the rest for a future pass. This was the favorite_count and retweet_count information from the API, and the first, most confident prediction from the image prediction data. I joined these columns to the enhanced archive by matching on tweet_id, and ignored the rest of those datasets. A deeper analysis would synthesize these more thoroughly.\n",
    "\n",
    "Otherwise, cleaning was mostly focused on datatype translating, null handling, and renaming. The most complicated process was combining the dog-stage data into a single column. This was further complicated by the discovery that some rows had multiple values for dog stage. Reading the provided data dictionary, I concluded that dog_stage should not be multivalued: the categories are meant to be exclusive. This allowed me to use the pandas combine_first method. Some data was lost in the process, but nothing that I think would impede the quality of an initial analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
